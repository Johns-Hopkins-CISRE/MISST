# 88888888888          88  88    dd        888888888888  88          88                  
# 88                   88        88             88       88                            
# 88           ,adPPYb,88  88  MM88MMM          88       88,dPYba,   88  ,adPPYba,                 
# 8888888     a8"    `Y88  88    88             88       88P    "8a  88  I8[    ""   
# 88          8b       88  88    88             88       88      88  88   `"Y8ba,        
# 88          "8a,   ,d88  88    88,            88       88      88  88  aa    ]8I   
# 88888888888  `"8bbdP"Y8  88    "Y888          88       88      88  88  `"YbbdP"'   
# ======= [The following parameters should be edited to fit your dataset] ========

# PreProcessor & Weight-Loading Parameters
edf_regex: '^(?!vent\.edf$).*\.edf$'
hypnogram_regex: '\bhynogram\.csv\b'
export_dir: data # Default is "data"
preprocess_data: True # Default is True, set as False to use pre-existing preprocessed data
annotations: # The annotations used to denote S0, S2, and REM sleep stages
  S0:  SLEEP-S0
  S2:  SLEEP-S2
  REM: SLEEP-REM
dataset_split: # The ratio of train, test, and validation sets
  TRAIN:  5
  TEST:   1
  VAL:    1
balance_ratios: # The distribution of classes within each split, "null" = "Do not balance"
  TRAIN: 
    S0:  null
    S2:  null 
    REM: null # 2, 3, 1
  TEST:
    S0:  null
    S2:  null 
    REM: null
  VAL:
    S0:  1
    S2:  1 
    REM: 1
channels: [EEG1, EEG2, EMGnu] # Names of PSG channels that will be used

# 888888ba,                              bb        888888888888                                  88                
# 88    `"8b                             88             88                                       88                       
# 88      `8b   ,adPPYba,   8b,dPYba,  MM88MMM          88   ,adPPYba,   88      88   ,adPPYba,  88,dPYba,         
# 88       88  a8"     "8a  88P'  `"8a   88             88  a8"     "8a  88      88  a8"     ""  88P'   "8a 
# 88      ,8P  8b       d8  88      88   88             88  8b       d8  88      88  8b          88      88 
# 88    .a8P   "8a,   ,a8"  88      88   88,            88  "8a,   ,a8"  "8a,  ,a88  "8a,   ,aa  88      88 
# 888888Y"'     `"YbbdP"'   88      88   "Y888          88   `"YbbdP"'    `"YbdP'Y8   `"Ybbd8"'  88      88
# ======================== [Don't change these unless you know what you're doing] =========================

# Training Specifications
mode: PLAIN # ["PLAIN", "DIST", "TUNER", "GUI", "DIST GUI", "TUNER GUI"]

# Automatic "model_param" Loading
load_tuned_archi: False # Default is True, set as False to manually configure "params" var
tuner_file_to_load: # Irrelevant if LOAD_TUNER_PARAMS is False
  tuner_type:  hyperband # ["hyperband", "bayesian"]
  tuned_param: model # ["model", "lr"]

# Manual "model_param" Specification
model_params:
  epochs:        600
  batch_size:    16
  learning_rate: 1.e-4 # 3.2e-4
  decay_steps:   70800 # 354*200
  alpha:         0.01
  optimizer:     adam # [Any Keras optimizer is valid (all lowercase)]
  model_type:    bottleneck # ["bottleneck", "sdcc"]

  archi_params:
    sdcc:
      filters:       6
      conv_layers:   5
      sdcc_blocks:   2

      lstm_nodes:    200
      lstm_layers:   2

      dense_nodes:   320
      dense_layers:  1
    bottleneck:
      init_kernel:    16

      cnn_blocks:     3 # 4
      bn_blocks:      2 # 3, Abbrev. for "Bottleneck"
      conv_pattern:   [1,3] # Mirrored: 1,3,4 -> 1,3,4,3,1
            
      filter_mult:    16  # Filter multiplier
      scaling_factor: 4  # Factor by which init val is multiplied and later compensation is applied

# "TunerTrainer"-specific Parameters
tuner_params:
  tuner_type:     hyperband # ["hyperband", "bayesian"] 
  params_to_tune: model # ["model", "lr"]
  goal:           val_sparse_categorical_accuracy
  dir_name:       tuner_results

  tuner_configs: 
    hyperband:
      max_epochs: 1
      factor:     3
    bayesian:
      num_trials: 20
